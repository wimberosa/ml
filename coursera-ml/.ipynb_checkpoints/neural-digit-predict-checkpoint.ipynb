{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data set and weight for the numbers we\n",
    "# will identify\n",
    "dataDict   = scipy.io.loadmat('./data/ex3data1.mat')\n",
    "weightDict = scipy.io.loadmat('./data/ex3weights.mat')\n",
    "\n",
    "X = dataDict['X']\n",
    "y = dataDict['y']\n",
    "\n",
    "thetaInput1 = weightDict['Theta1']\n",
    "thetaInput2 = weightDict['Theta2']\n",
    "\n",
    "# Define number of layers (3) and number of nodes in each layer\n",
    "s = OrderedDict()\n",
    "s[1] = X.shape[1] # 20x20 input image (400 nodes in layer 1)\n",
    "s[2] = 25         # hidden layer (25 nodes in layer 2)\n",
    "s[3] = 10         # digits from 1 thru 10 (10 nodes in output layer 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randInitialWeights(rows, cols):\n",
    "    # Define and randomize theta matrices\n",
    "    # One effective strategy for choosing epsilon is to base it on the number of units in the \n",
    "    # network. A good choice of epsilon is (sqrt(6) / sqrt(Lin + Lout)), where Lin = sl \n",
    "    # and Lout = sl+1 are the number of units in the layers adjacent to Î˜(l).\n",
    "    epsilon = np.sqrt(6) / np.sqrt(rows + cols)\n",
    "    return np.random.rand(rows, cols) * 2 * epsilon - epsilon\n",
    "\n",
    "# Theta at layer 1 is 25 rows (activation output) by 401 input columns\n",
    "theta1 = randInitialWeights(s[2], s[1]+1)\n",
    "\n",
    "# Theta at layer 2 is 10 rows (activation output) by 26 input columns\n",
    "theta2 = randInitialWeights(s[3], s[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data keys dict_keys(['__globals__', 'X', '__header__', 'y', '__version__'])\n",
      "weight keys dict_keys(['__globals__', 'Theta1', '__header__', 'Theta2', '__version__'])\n",
      "X shape=(5000, 400)\n",
      "y shape=(5000, 1)\n",
      "Theta1 shape=(25, 401)\n",
      "Theta2 shape=(10, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"data keys\", dataDict.keys())\n",
    "print(\"weight keys\", weightDict.keys())\n",
    "print(\"X shape={0}\\ny shape={1}\".format(X.shape, y.shape))\n",
    "print(\"Theta1 shape={0}\\nTheta2 shape={1}\".format(theta1.shape, theta2.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DisplayFunc(imageArray, startRow, displayRows, displayCols):\n",
    "    ''' \n",
    "    imageArray  : vectorized rows of m X n image\n",
    "    startRow    : row to start from in imageArray\n",
    "    displayRows : number of rows to display on screen\n",
    "    displayCols : number of columns to display on screen\n",
    "    '''\n",
    "    m, n = imageArray.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(2. * displayCols, 2.26 * displayRows))\n",
    "    gs = gridspec.GridSpec(displayRows, displayCols, wspace=0.0025, hspace=0.0025)\n",
    "\n",
    "    ax = [plt.subplot(gs[i]) for i in range(displayRows * displayCols)]\n",
    "    gs.update(hspace=0)\n",
    "    gs.tight_layout(fig, pad=0, h_pad=0, w_pad=0)\n",
    "\n",
    "    for i, im in enumerate(imageArray[startRow:startRow + displayRows * displayCols]):\n",
    "        vmax = max(im.max(), -im.min())\n",
    "        ax[i].imshow(im.reshape(20,20), cmap=plt.cm.gray,\n",
    "                     interpolation='nearest', vmin=-vmax, vmax=vmax)\n",
    "        ax[i].axis('off')\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].set_yticklabels([])\n",
    "        ax[i].set_aspect('equal')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \n",
    "    def __init__(self, X, y, theta1, theta2, rlambda):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        \n",
    "        # Use the one hot encoder or the alternate method following\n",
    "        # the encoder\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        self._yhot = encoder.fit_transform(y)\n",
    "\n",
    "        # Each y for every sample turns into a vector with 10 elements\n",
    "        # equating to the 10 output nodes of the neural network.  If \n",
    "        # y == 1 then yhot == [ 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        # y == 2 then yhot == [ 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        # y == 0 then yhot == [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        # etc.\n",
    "        # Note that the ymod row is still the same index as the\n",
    "        # equivalent x row sample.\n",
    "        #\n",
    "        #self._yhot = np.zeros((y.shape[0], 10)) # most of them are zeros\n",
    "        #for i, val in enumerate(y):\n",
    "        #   self._yhot[i][val-1] = 1\n",
    "            \n",
    "        self._m        = X.shape[0] # number of samples\n",
    "        self._theta1   = theta1\n",
    "        self._theta2   = theta2\n",
    "        self._rlambda  = rlambda    # regularization lambda (0 for no regularization)\n",
    "        \n",
    "    def forwardPropogate(self, xinput):\n",
    "        \n",
    "        # At column 0 insert a Ones column =>  a1.shape=(5000, 401)\n",
    "        a1 = np.insert(xinput, 0, 1, axis=1)\n",
    "        \n",
    "        #  (5000 X 401) dot transpose(25 x 401) => z2.shape=(5000, 25)\n",
    "        z2 = np.dot(a1, self._theta1.T)\n",
    "        \n",
    "        a2 = 1 / (1 + np.exp(-z2))\n",
    "        \n",
    "        # At column 0 insert a Ones column => a2.shape=(5000, 26)\n",
    "        a2 = np.insert(a2, 0, 1, axis=1)\n",
    "        \n",
    "        # (5000 X 26) dot transpose(10 X 26) => z3.shape=(5000, 10)\n",
    "        z3 = np.dot(a2, self._theta2.T)\n",
    "        \n",
    "        # h.shape = (5000 X 10)\n",
    "        h = 1 / (1 + np.exp(-z3))\n",
    "        \n",
    "        return a1, a2, h\n",
    "        \n",
    "    def computeCost(self):        \n",
    "              \n",
    "        a1, a2, h = self.forwardPropogate(self._X)\n",
    "        \n",
    "        sum = 0\n",
    "        for i in range(0, self._m):\n",
    "            \n",
    "            # (1 X 10) dot transpose(1 X 10) = (1 X 1)\n",
    "            sum += (-np.dot(    self._yhot[i:i+1], np.transpose(np.log(    h[i:i+1]))) -\n",
    "                     np.dot(1 - self._yhot[i:i+1], np.transpose(np.log(1 - h[i:i+1]))))\n",
    "        \n",
    "        sum  = sum / self._m\n",
    "        sum += self._rlambda / (2 * self._m) * (np.sum(np.square(self._theta1[:][1:])) + \n",
    "                                                np.sum(np.square(self._theta2[:][1:])))\n",
    "        return sum\n",
    "    \n",
    "    # end of computeCost\n",
    "\n",
    "    def gradientDescent(self, alpha, iters):\n",
    "        \n",
    "        # how many thetas are we computing. Theta is\n",
    "        # a column vector\n",
    "        ntheta = self._theta.shape[0]\n",
    "        \n",
    "        cost = np.zeros(iters)\n",
    "        \n",
    "        # for maximum number of gradient descent\n",
    "        # iterations\n",
    "        for ndescent in range(iters):\n",
    "            \n",
    "            # hypothesis is:\n",
    "            #   h_theta(x) = g(theta(0) * x(0) + theta(1) * x(1) + ... + theta(n) * x(n))\n",
    "            #\n",
    "            # g is the sigmoid function:\n",
    "            #   g(z) = 1 / (1 + exp(-z))\n",
    "            #\n",
    "            inner = np.dot(self._X, self._theta)\n",
    "            \n",
    "            # This is the sigmoid function giving us values between 0 and 1\n",
    "            h_theta_x = 1 / (1 + np.exp(-inner))\n",
    "                 \n",
    "            n1 = h_theta_x - self._y\n",
    "                        \n",
    "            # _X of 5000 samples would be (5000 x 1))\n",
    "            n2 = np.dot(np.transpose(self._X[:,0:1]), n1)\n",
    "            \n",
    "            n3 = alpha * (1 / self._m) * n2\n",
    "            self._theta[0] = self._theta[0] - n3\n",
    "            #print(\"shape n1=\", n1.shape, \"n2=\", n2.shape, \"n3=\", n3.shape)\n",
    "          \n",
    "            # _X of 5000 samples and 20 features would be (5000 x (20 - 1))\n",
    "            n2 = np.dot(np.transpose(self._X[:,1:]), n1)\n",
    "            \n",
    "            n3 = alpha * ((1 / self._m) * n2 + (self._rlambda / self._m) * self._theta[1:])\n",
    "            self._theta[1:] = self._theta[1:] - n3\n",
    "            #print(\"shape n1=\", n1.shape, \"n2=\", n2.shape, \"n3=\", n3.shape)\n",
    "            \n",
    "            cost[ndescent] = self.computeCost()\n",
    "        # end for ndescent\n",
    "\n",
    "        return self._theta, cost\n",
    "    # end gradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda= 0 cost= [[ 0.28762917]]\n",
      "lambda= 1.0 cost= [[ 0.3790464]]\n",
      "Samples= 5000 Prediction errors= 124 Success rate= 0.9752\n"
     ]
    }
   ],
   "source": [
    "m = X.shape[0]\n",
    "errCount = 0\n",
    "alpha    = 0.001\n",
    "rlambda  = 1.0\n",
    "\n",
    "# compute cost with no regularization\n",
    "cost = NeuralNet(X, y, thetaInput1, thetaInput2, 0).computeCost()\n",
    "print(\"lambda=\", 0, \"cost=\", cost)\n",
    "\n",
    "# compute cost with regularization\n",
    "cost = NeuralNet(X, y, thetaInput1, thetaInput2, rlambda).computeCost()\n",
    "print(\"lambda=\", rlambda, \"cost=\", cost)\n",
    "\n",
    "# Now start using our unlearned thetas\n",
    "gd = NeuralNet(X, y, thetaInput1, thetaInput2, rlambda)\n",
    "\n",
    "# Now lets check out our prediction versus actual\n",
    "a1, a2, h = gd.forwardPropogate(X)\n",
    "\n",
    "# Python arrays are indexed by zero so\n",
    "# we'll need to add 1 for comparison to y\n",
    "for i in range(0, m):\n",
    "    imax = np.where(h[i] == h[i].max())\n",
    "    if y[i] != imax[0]+1:\n",
    "        errCount += 1\n",
    "        \n",
    "print(\"Samples=\", m, \"Prediction errors=\", errCount, \"Success rate=\", (m-errCount)/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
